大数据算命系列（1）
===================

朴素贝叶斯算法简介与应用
~~~~~~~~~~~~~~~~~~~~~~~~

     -----  大数据算命师

     -----  2013.06.06(更新:2013.12.13)


--------------------------------------------------------------------------------

贝叶斯公式
==========

* 贝叶斯公式
.. image:: ./bayes.png

* Paul Graham
    这种过滤器还具有自我学习的功能，会根据新收到的邮件，不断调整。收到的垃圾邮件越多，它的准确率就越高

* 分词技术
    1. 中文分词
    #. 特定的url分词

----------------------------------------------------------------------------------------------


几个概率
========

* 先验概率
   在没有任何前提下， 事件A发生的概率

* 条件概率
   在条件A发生前提下， 事件A的概率

* 联合概率
   在多个事件发生的情况下，另一个事件发生概率有多大。

   比如，已知W1和W2是两个不同的词语，它们都出现在某封电子邮件之中，那么这封邮件是垃圾邮件的概率，就是联合概率。

----------------------------------------------------------------------------------------------

两大算法
========

* 分类

   把特征非常明显的东西分成几类
   代表算法：bayes
   应用：垃圾邮件过滤，攻击识别

* 聚类

   把特征非常相似的东西分聚合在一起
   代表算法：kmeans
   应用： 明星三维身材聚类
   
   

----------------------------------------------------------------------------------------------


数据结构
==============

.. code-block:: python

   {
	   "_id" : ObjectId("52aa80ad98e86672bad74173"),
	   "attack_info" : {
	   "count" : 132,
	   "prob" : 0.003693857562613684
	   },
	   "access_info" : {
	   "count" : 0,
	   "prob" : 0
	   },
	   "word" : "t(",
	   "prob" : 0.9999
   }

说明：

1. 单词为t(
2. 这个词被识别和攻击的概率为0.999
3. 在攻击日志中出现次数为132次，概率为0.003693857562613684
4. 在正常日志中出现次数为0次，概率为0

----------------------------------------------------------------------------------------------


Paul Graham的计算公式假设
=========================

1. 在样本中，如果某个词只出现在垃圾邮件中，它在正常邮件的出现频率是1%，反之亦然。这里Paul Grahm认为有改善的空间。

2. 假定垃圾邮件的"先验概率"为50%。文中的样本也是由4000份垃圾邮件和4000份正常邮件组成。

3. 如果有的词是第一次出现，无法计算P(S|W)，就假定这个值等于0.4。


----------------------------------------------------------------------------------------------


概率公式
========

条件概率：
~~~~~~~~~~

.. sourcecode:: python

    prob[k]["good_prob"] = good_prob
    prob[k]["bad_prob"] = bad_prob
    prob[k]["prob"] = bad_prob/(good_prob+bad_prob)

联合概率：
~~~~~~~~~~

.. sourcecode:: python

    # 复合概率公式
    #                          P1*P2*P3*...*Pn
    # P = -----------------------------------------------------------
    #       (P1*P2*P3*...*Pn + (1-P1)*(1-P2)*(1-P3)*...*(1-Pn))

    # mutil_prob: P1*P2*P3*...*Pn
    # onesub_muti_prob: (1-P1)*(1-P2)*(1-P3)*...*(1-Pn)

    mutil_prob = 1
    onesub_muti_prob = 1
    for w in words:
        prob = prob_dict[w]["prob"]
        print "[%s] [%s]" % (w, prob, )
        mutil_prob *= prob
        onesub_muti_prob *= (1-prob)

    attack_probability = mutil_prob/(mutil_prob + onesub_muti_prob)

    return attack_probability

----------------------------------------------------------------------------------------------


url分词研究(1)
==============

原则
~~~~

1. 只按空格进行分割。
#. 最后的多个空格，进行压缩后只需要一个空格。
#. 要实现攻击的目的，必须要使用特殊符号，关键字，函数名。
#. 特殊符号能非常有效的匹配正常请求与攻击请求。

分词技术
~~~~~~~~

1. 将所有的特殊符号替换为： 空格+特殊符号+空格
#. 将部分符号由1变成2, 这样可以将符号往前面都靠上去
#. 符号集 / ( ) . =
#. / 主要代表了路径
#. ( ) 通常是和函数一起
#. . 按文件名与文件后缀进行侵害
#. = 变量名，变量值

----------------------------------------------------------------------------------------------

url分词研究(2)
==============

1. 将部分符号只往左靠才有意义

   1. > ,
   #. script> select null,1,3,5,
   #. 1>2  # 大于符号要算例外 

#. 部分符号只往右靠才有意义

   1. & ? @ <
   #. 小于符号算例外
   #. @@version ?id=  &cm=  <script

#. 要解决，被同时向右靠和同时向左靠

   1. [Access] [/one.php?id=1&uid=3.php;.jpg]
   #. [Split]  / /one. .php ?id= =1 &uid= =3. .php ;. .jpg
   #. 其中的=3.这个分词，3就是被向右靠的=与向左千的.

#. 将有意义的符号组合拼接起来

   1. /**/# sql注释   --$# sql注释
   #. /../ # 目录跳转
   #. http:// # http网站，远程包含，url跳转

----------------------------------------------------------------------------------------------


整个流程
========

1. 训练数据
    
  1. 越准备越好
  2. 可由人工来构造，也可以由其它的算法得出
  3. 这步构建好模型

2. 测试数据

  1. 使用上面的模型来测试，应用于实际
  2. 在测试的时候，自学习出现的新词汇，反馈并更新模型
  3. 人工干预，修正模型

----------------------------------------------------------------------------------------------

how && why
============

how
~~~

1. 熟悉整个算法流程，并理解其它中各个细节

2. 构建分类纯净的训练数据

3. 不断研究分词技术（重点），并查看对结果的影响

4. 反复进行最上面三个步骤

why
---

1. 如果你的过滤器效果越好，就越不能出现误判，一旦误判，后果就会变得很严重。

   类似于: 富人越富，穷人越穷


2. 如果你发现过滤器效果出问题了，有正常的请求被识别成了攻击，那么手动调整，或检查原因

   类似于: 布施，法律

----------------------------------------------------------------------------------------------


Thank you!
==========
